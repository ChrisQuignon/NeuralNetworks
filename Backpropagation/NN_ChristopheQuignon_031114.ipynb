{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#2.\n",
      "def g(z):\n",
      "    return (1/(1+exp(-1*z)))\n",
      "\n",
      "def g_dash(z):\n",
      "    return g(z)*(1-g(z))\n",
      "\n",
      "class ffn():\n",
      "    def __init__(self):\n",
      "        \n",
      "        self.v_input = numpy.array([1, -1])\n",
      "        \n",
      "        self.w_hidden = numpy.array([[0.3, -0.2], [0.2, -0.1]])\n",
      "        self.v_hidden = numpy.array([1, -1])\n",
      "        \n",
      "        self.w_output = numpy.array([1, -1])\n",
      "        self.v_output = numpy.array([1]);\n",
      "        \n",
      "        \n",
      "        self.delta_output = numpy.array([0])\n",
      "        self.delta_hidden = numpy.array([0])\n",
      "        \n",
      "        self.learning_rate = 0.2\n",
      "        \n",
      "        self.desired_output = 1\n",
      "        self.epochs = 100\n",
      "    \n",
      "    def forward(self):\n",
      "        self.v_hidden = dot(self.v_input, self.w_hidden)\n",
      "        self.v_output = dot(self.v_hidden, self.w_output)                \n",
      "    \n",
      "    \n",
      "    def error(self):\n",
      "        error = g(self.v_output) - self.desired_output\n",
      "        \n",
      "        self.delta_output = g_dash(self.v_output) * error\n",
      "        self.delta_hidden = g_dash(self.v_output) * dot(self.delta_output, self.w_output)\n",
      "    \n",
      "    def backwards(self):\n",
      "        self.w_hidden =  self.w_hidden - self.learning_rate * dot(self.delta_hidden, self.v_input)\n",
      "\n",
      "        self.w_output =  self.w_output-self.learning_rate * dot(self.delta_output, self.v_hidden)\n",
      "           \n",
      "f = ffn()\n",
      "print ''\n",
      "print 'errors'\n",
      "print f.delta_output\n",
      "print f.delta_hidden\n",
      "print 'weights'\n",
      "print f.w_hidden\n",
      "print f.w_output\n",
      "print 'values:'\n",
      "print f.v_input\n",
      "print f.v_hidden\n",
      "print f.v_output\n",
      "\n",
      "for i in range(2000):\n",
      "    f.forward()    \n",
      "    f.error()\n",
      "    f.backwards()\n",
      "    \n",
      "print ''\n",
      "print 'errors:'\n",
      "print f.delta_hidden\n",
      "print f.delta_output\n",
      "print 'weights:'\n",
      "print f.w_hidden\n",
      "print f.w_output\n",
      "print 'values:'\n",
      "print f.v_input\n",
      "print f.v_hidden\n",
      "print f.v_output\n",
      "\n",
      "#does not converge\n",
      "#does not work due to missing bias\n",
      "#random weights should converge faster\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "errors\n",
        "[0]\n",
        "[0]\n",
        "weights\n",
        "[[ 0.3 -0.2]\n",
        " [ 0.2 -0.1]]\n",
        "[ 1 -1]\n",
        "values:\n",
        "[ 1 -1]\n",
        "[ 1 -1]\n",
        "[1]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "errors:\n",
        "[-0.05562387  0.05562387]\n",
        "-0.061486245233\n",
        "weights:\n",
        "[[ 40.93585552  40.43585552]\n",
        " [ 40.83585552  40.53585552]]\n",
        "[ 4.34758085 -4.34758085]\n",
        "values:\n",
        "[ 1 -1]\n",
        "[ 0.1 -0.1]\n",
        "0.869270225145\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#3.\n",
      "from pybrain.supervised.trainers import BackpropTrainer\n",
      "from pybrain.tools.shortcuts import buildNetwork\n",
      "from pybrain.datasets import SupervisedDataSet\n",
      "from numpy import arange\n",
      "from random import uniform\n",
      "from math import pi\n",
      "\n",
      "funs = []\n",
      "funs.append([lambda x: 1/x, 1.0, 100.0])\n",
      "funs.append([lambda x: math.log(x, 10), 1.0, 10.0])\n",
      "funs.append([lambda x: math.exp(-1*x), 1.0, 10.0])\n",
      "funs.append([lambda x: sin(x), 0.0, pi/2])\n",
      "\n",
      "def accuracytest(f, min, max):\n",
      "    d = 0\n",
      "    rounds = 100\n",
      "    max_hidden = 6\n",
      "    epochs = 100\n",
      "    r = []\n",
      "    \n",
      "    for n in range(0, max_hidden):\n",
      "        net = buildNetwork(1, n+1, 1, bias=True)\n",
      "        ds = SupervisedDataSet(1, 1)\n",
      "        #fill samples\n",
      "        for x in [uniform(min, max) for _ in range(rounds)]:\n",
      "            ds.addSample(x, f(x))\n",
      "        \n",
      "        trainer = BackpropTrainer(net, ds)\n",
      "        trainer.trainEpochs(epochs)\n",
      "        \n",
      "        #test\n",
      "        #take samples\n",
      "        for x in [uniform(min, max) for _ in range(rounds)]:\n",
      "            #return the average absolute error\n",
      "            d = d+ abs((net.activate([x])-f(x)))/rounds\n",
      "        r.append((n, d))\n",
      "    return r\n",
      "\n",
      "#Run tests\n",
      "for f in funs:\n",
      "    print 'Function ', f[0], ':'\n",
      "    deltas = accuracytest(f[0],f[1],f[2])\n",
      "    for r in deltas:\n",
      "        print 'Accuracy with ', r[0], ' hidden neurons: ', 1-r[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Function  <function <lambda> at 0x44abde8> :\n",
        "Accuracy with "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0  hidden neurons:  [ 0.96736017]\n",
        "Accuracy with  1  hidden neurons:  [ 0.95373111]\n",
        "Accuracy with  2  hidden neurons:  [ 0.92915474]\n",
        "Accuracy with  3  hidden neurons:  [ 0.87968814]\n",
        "Accuracy with  4  hidden neurons:  [ 0.86140591]\n",
        "Accuracy with  5  hidden neurons:  [ 0.85404965]\n",
        "Function  <function <lambda> at 0x44abaa0> :\n",
        "Accuracy with "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0  hidden neurons:  [ 0.95919169]\n",
        "Accuracy with  1  hidden neurons:  [ 0.73091643]\n",
        "Accuracy with  2  hidden neurons:  [ 0.71095567]\n",
        "Accuracy with  3  hidden neurons:  [ 0.7011763]\n",
        "Accuracy with  4  hidden neurons:  [ 0.66539962]\n",
        "Accuracy with  5  hidden neurons:  [ 0.64178554]\n",
        "Function  <function <lambda> at 0x44abed8> :\n",
        "Accuracy with "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0  hidden neurons:  [ 0.94999978]\n",
        "Accuracy with  1  hidden neurons:  [ 0.94151966]\n",
        "Accuracy with  2  hidden neurons:  [ 0.89339914]\n",
        "Accuracy with  3  hidden neurons:  [ 0.8787871]\n",
        "Accuracy with  4  hidden neurons:  [ 0.86514959]\n",
        "Accuracy with  5  hidden neurons:  [ 0.84691531]\n",
        "Function  <function <lambda> at 0x44abd70> :\n",
        "Accuracy with "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0  hidden neurons:  [ 0.92821316]\n",
        "Accuracy with  1  hidden neurons:  [ 0.90580606]\n",
        "Accuracy with  2  hidden neurons:  [ 0.89129798]\n",
        "Accuracy with  3  hidden neurons:  [ 0.87249108]\n",
        "Accuracy with  4  hidden neurons:  [ 0.83680569]\n",
        "Accuracy with  5  hidden neurons:  [ 0.82931626]\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#The accuracy drops with more hidden neurons"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}