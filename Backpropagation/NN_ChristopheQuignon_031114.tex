

%% This file was auto-generated by IPython.
%% Conversion from the original notebook file:
%%
\documentclass[11pt,english]{article}

%% This is the automatic preamble used by IPython.  Note that it does *not*
%% include a documentclass declaration, that is added at runtime to the overall
%% document.

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

% Scale down larger images
\usepackage[export]{adjustbox}

%fancy verbatim
\usepackage{fancyvrb}
% needed for markdown enumerations to work
\usepackage{enumerate}

% Slightly bigger margins than the latex defaults
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}

% Define a few colors for use in code, links and cell shading
\usepackage{color}
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}
\definecolor{myteal}{rgb}{.26, .44, .56}
\definecolor{gray}{gray}{0.45}
\definecolor{lightgray}{gray}{.95}
\definecolor{mediumgray}{gray}{.8}
\definecolor{inputbackground}{rgb}{.95, .95, .85}
\definecolor{outputbackground}{rgb}{.95, .95, .95}
\definecolor{traceback}{rgb}{1, .95, .95}

% new ansi colors
\definecolor{brown}{rgb}{0.54,0.27,0.07}
\definecolor{purple}{rgb}{0.5,0.0,0.5}
\definecolor{darkgray}{gray}{0.25}
\definecolor{lightred}{rgb}{1.0,0.39,0.28}
\definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
\definecolor{lightblue}{rgb}{0.53,0.81,0.92}
\definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
\definecolor{lightcyan}{rgb}{0.5,1.0,0.83}

% Framed environments for code cells (inputs, outputs, errors, ...).  The
% various uses of \unskip (or not) at the end were fine-tuned by hand, so don't
% randomly change them unless you're sure of the effect it will have.
\usepackage{framed}

% remove extraneous vertical space in boxes
\setlength\fboxsep{0pt}

% codecell is the whole input+output set of blocks that a Code cell can
% generate.

% TODO: unfortunately, it seems that using a framed codecell environment breaks
% the ability of the frames inside of it to be broken across pages.  This
% causes at least the problem of having lots of empty space at the bottom of
% pages as new frames are moved to the next page, and if a single frame is too
% long to fit on a page, will completely stop latex from compiling the
% document.  So unless we figure out a solution to this, we'll have to instead
% leave the codecell env. as empty.  I'm keeping the original codecell
% definition here (a thin vertical bar) for reference, in case we find a
% solution to the page break issue.

%% \newenvironment{codecell}{%
%%     \def\FrameCommand{\color{mediumgray} \vrule width 1pt \hspace{5pt}}%
%%    \MakeFramed{\vspace{-0.5em}}}
%%  {\unskip\endMakeFramed}

% For now, make this a no-op...
\newenvironment{codecell}{}

 \newenvironment{codeinput}{%
   \def\FrameCommand{\colorbox{inputbackground}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\endMakeFramed}

\newenvironment{codeoutput}{%
   \def\FrameCommand{\colorbox{outputbackground}}%
   \vspace{-1.4em}
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\unskip\medskip\endMakeFramed}

\newenvironment{traceback}{%
   \def\FrameCommand{\colorbox{traceback}}%
   \MakeFramed{\advance\hsize-\width \FrameRestore}}
 {\endMakeFramed}

% Use and configure listings package for nicely formatted code
\usepackage{listingsutf8}
\lstset{
  language=python,
  inputencoding=utf8x,
  extendedchars=\true,
  aboveskip=\smallskipamount,
  belowskip=\smallskipamount,
  xleftmargin=2mm,
  breaklines=true,
  basicstyle=\small \ttfamily,
  showstringspaces=false,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{myteal},
  stringstyle=\color{darkgreen},
  identifierstyle=\color{darkorange},
  columns=fullflexible,  % tighter character kerning, like verb
}

% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
\usepackage{hyperref}
\hypersetup{
  breaklinks=true,  % so long urls are correctly broken across lines
  colorlinks=true,
  urlcolor=blue,
  linkcolor=darkorange,
  citecolor=darkgreen,
  }

% hardcode size of all verbatim environments to be a bit smaller
\makeatletter 
\g@addto@macro\@verbatim\small\topsep=0.5em\partopsep=0pt
\makeatother 

% Prevent overflowing lines due to urls and other hard-to-break entities.
\sloppy




\begin{document}



\begin{codecell}


\begin{codeinput}
\begin{lstlisting}
#2.
def g(z):
    return (1/(1+exp(-1*z)))

def g_dash(z):
    return g(z)*(1-g(z))

class ffn():
    def __init__(self):
        
        self.v_input = numpy.array([1, -1])
        
        self.w_hidden = numpy.array([[0.3, -0.2], [0.2, -0.1]])
        self.v_hidden = numpy.array([1, -1])
        
        self.w_output = numpy.array([1, -1])
        self.v_output = numpy.array([1]);
        
        
        self.delta_output = numpy.array([0])
        self.delta_hidden = numpy.array([0])
        
        self.learning_rate = 0.2
        
        self.desired_output = 1
        self.epochs = 100
    
    def forward(self):
        self.v_hidden = dot(self.v_input, self.w_hidden)
        self.v_output = dot(self.v_hidden, self.w_output)                
    
    
    def error(self):
        error = g(self.v_output) - self.desired_output
        
        self.delta_output = g_dash(self.v_output) * error
        self.delta_hidden = g_dash(self.v_output) * dot(self.delta_output, self.w_output)
    
    def backwards(self):
        self.w_hidden =  self.w_hidden - self.learning_rate * dot(self.delta_hidden, self.v_input)

        self.w_output =  self.w_output-self.learning_rate * dot(self.delta_output, self.v_hidden)
           
f = ffn()
print ''
print 'errors'
print f.delta_output
print f.delta_hidden
print 'weights'
print f.w_hidden
print f.w_output
print 'values:'
print f.v_input
print f.v_hidden
print f.v_output

for i in range(2000):
    f.forward()    
    f.error()
    f.backwards()
    
print ''
print 'errors:'
print f.delta_hidden
print f.delta_output
print 'weights:'
print f.w_hidden
print f.w_output
print 'values:'
print f.v_input
print f.v_hidden
print f.v_output

#does not converge
#does not work due to missing bias
#random weights should converge faster

\end{lstlisting}
\end{codeinput}
\begin{codeoutput}


\begin{Verbatim}[commandchars=\\\{\}]
errors
[0]
[0]
weights
[[ 0.3 -0.2]
 [ 0.2 -0.1]]
[ 1 -1]
values:
[ 1 -1]
[ 1 -1]
[1]

errors:
[-0.05562387  0.05562387]
-0.061486245233
weights:
[[ 40.93585552  40.43585552]
 [ 40.83585552  40.53585552]]
[ 4.34758085 -4.34758085]
values:
[ 1 -1]
[ 0.1 -0.1]
0.869270225145
\end{Verbatim}

\end{codeoutput}

\end{codecell}

\begin{codecell}


\begin{codeinput}
\begin{lstlisting}
#3.
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.datasets import SupervisedDataSet
from numpy import arange
from random import uniform
from math import pi

funs = []
funs.append([lambda x: 1/x, 1.0, 100.0])
funs.append([lambda x: math.log(x, 10), 1.0, 10.0])
funs.append([lambda x: math.exp(-1*x), 1.0, 10.0])
funs.append([lambda x: sin(x), 0.0, pi/2])

def accuracytest(f, min, max):
    d = 0
    rounds = 100
    max_hidden = 6
    epochs = 100
    r = []
    
    for n in range(0, max_hidden):
        net = buildNetwork(1, n+1, 1, bias=True)
        ds = SupervisedDataSet(1, 1)
        #fill samples
        for x in [uniform(min, max) for _ in range(rounds)]:
            ds.addSample(x, f(x))
        
        trainer = BackpropTrainer(net, ds)
        trainer.trainEpochs(epochs)
        
        #test
        #take samples
        for x in [uniform(min, max) for _ in range(rounds)]:
            #return the average absolute error
            d = d+ abs((net.activate([x])-f(x)))/rounds
        r.append((n, d))
    return r

#Run tests
for f in funs:
    print 'Function ', f[0], ':'
    deltas = accuracytest(f[0],f[1],f[2])
    for r in deltas:
        print 'Accuracy with ', r[0], ' hidden neurons: ', 1-r[1]
\end{lstlisting}
\end{codeinput}
\begin{codeoutput}


\begin{Verbatim}[commandchars=\\\{\}]
Function  <function <lambda> at 0x44abde8> :
Accuracy with  0  hidden neurons:  [ 0.96736017]
Accuracy with  1  hidden neurons:  [ 0.95373111]
Accuracy with  2  hidden neurons:  [ 0.92915474]
Accuracy with  3  hidden neurons:  [ 0.87968814]
Accuracy with  4  hidden neurons:  [ 0.86140591]
Accuracy with  5  hidden neurons:  [ 0.85404965]
Function  <function <lambda> at 0x44abaa0> :
Accuracy with  0  hidden neurons:  [ 0.95919169]
Accuracy with  1  hidden neurons:  [ 0.73091643]
Accuracy with  2  hidden neurons:  [ 0.71095567]
Accuracy with  3  hidden neurons:  [ 0.7011763]
Accuracy with  4  hidden neurons:  [ 0.66539962]
Accuracy with  5  hidden neurons:  [ 0.64178554]
Function  <function <lambda> at 0x44abed8> :
Accuracy with  0  hidden neurons:  [ 0.94999978]
Accuracy with  1  hidden neurons:  [ 0.94151966]
Accuracy with  2  hidden neurons:  [ 0.89339914]
Accuracy with  3  hidden neurons:  [ 0.8787871]
Accuracy with  4  hidden neurons:  [ 0.86514959]
Accuracy with  5  hidden neurons:  [ 0.84691531]
Function  <function <lambda> at 0x44abd70> :
Accuracy with  0  hidden neurons:  [ 0.92821316]
Accuracy with  1  hidden neurons:  [ 0.90580606]
Accuracy with  2  hidden neurons:  [ 0.89129798]
Accuracy with  3  hidden neurons:  [ 0.87249108]
Accuracy with  4  hidden neurons:  [ 0.83680569]
Accuracy with  5  hidden neurons:  [ 0.82931626]
\end{Verbatim}

\end{codeoutput}

\end{codecell}

\begin{codecell}


\begin{codeinput}
\begin{lstlisting}
#The accuracy drops with more hidden neurons
\end{lstlisting}
\end{codeinput}

\end{codecell}



\end{document}

